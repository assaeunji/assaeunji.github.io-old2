---
layout: post
title: "나무 알고리즘: 랜덤 포레스트에서 XGBoost까지"
date: 
categories: []
tag: []
comments: true
---



---
## Introduction

이 포스트에서 살펴볼 나무 알고리즘은 
  * Random Forest
  * Gradient Boosting 
  * Light GBM (Light Gradient Boost)
  * XGBoost (eXtra Gradient Boost)
로 4개의 알고리즘입니다. 이 알고리즘들은 모두 **앙상블 학습**에 해당하는데요.

**앙상블 학습** (Ensemble Learning)이란 여러 개의 분류기 (Classifier)를 생성하고 그 예측을 결합해 단일 분류기보다 정확한 최종적 예측을 도출해내는 기법을 의미합니다.

앙상블 학습의 유형은 크게 세 가지입니다 (그 외에도 스태킹을 포함한 다양한 앙상블 방법이 있다고 합니다).
* 보팅 (Voting)
* 배깅 (Bagging)
* 부스팅 (Boosting) 

제가 살펴볼 4가지 나무 알고리즘 중 랜덤 포레스트 (Random Forest)는 **배깅**, 나머지 Gradient Boosting, Light GBM, XGBoost는 모두 **부스팅**을 이용한 방법입니다.

그렇다면 배깅, 부스팅은 어떤 방법을 의미할까요?

----
## Bagging and Boosting

먼저 **배깅 (Bagging)**은 Bootstrap + Aggregating의 합성어로, "부트스트랩 샘플을 합친다!"라고 한 마디로 말씀드릴 수 있습니다. **부트스트랩**이란 원래 통계학에서 표본 분포 (Sampling Distribution)을 구하기 위해 데이터를 여러 번 복원 추출 (resampling)하는 것을 의미합니다.

평균이 $\overline{X}$이고, 분산이 $\sigma^2/n$인 서로 독립인 $n$개의 데이터 $X_1,\ldots X_n$가 있을 때 이를 하나의 표본 (samples)이라 부릅니다. 그리고 보통 모집단 (population)의 모평균과 모분산을 추정하기 위해 표본 분포를 구합니다.
근데 표본 분포를 구하려면 또 $n$개의 표본들을 여러 개 구해야 하는데 사실상 쉽지가 않은 과정이죠.

이런 한계를 극복하기 위해 나온 과정이 부트스트랩입니다. 하나의 표본을 가지고 $B$ 번 복원 추출 (resampling) 한다면 이 **부트스트랩 샘플들의 분포가 표본 분포로 근사된다**는 것이 핵심입니다.

자, 그럼 부트스트랩 샘플들을 어떻게 추출하는 지 그림을 통해 알아봅시다.
예를 들어 1,2,3,...,0까지의 값을 가진 $n=10$개인 하나의 표본이 있을 때 이 표본으로 $B=3$개의 부트스트랩 샘플을 추출하면 다음과 같습니다.

![](../../images/tree-bootstrap.png)

오른쪽의 세 개의 부트스트랩 샘플들은 왼쪽의 데이터 개수와 동일하지만, 개별 데이터가 중복되는 특징이 있습니다. 이렇게 
* 표본 크기와 똑같이 $B$번
* 복원 추출 (with replacement)
하는 것이 부트스트랩 샘플의 핵심입니다.

이러한 철학을 나무 모형에 적용한 예가 **랜덤 포레스트 (Random Forest)**입니다.

