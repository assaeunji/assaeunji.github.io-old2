---
layout: post
title: "배깅 (Bagging)과 부스팅 (Boosting)의 원리 톺아보기"
date: 
categories: []
tag: []
comments: true
---



---
## Introduction

배깅과 부스팅을 알아보기 전에 **앙상블 학습**에 대한 개념을 알아야 합니다. 
**앙상블 학습** (Ensemble Learning)이란 여러 개의 분류기 (Classifier)를 생성하고 그 예측을 결합해 단일 분류기보다 정확한 최종적 예측을 도출해내는 기법을 의미합니다.

앙상블 학습의 유형은 크게 세 가지입니다 (그 외에도 스태킹을 포함한 다양한 앙상블 방법이 있다고 합니다).
* 보팅 (Voting)
* 배깅 (Bagging)
* 부스팅 (Boosting) 

나무 알고리즘 중 랜덤 포레스트 (Random Forest)는 **배깅**, 나머지 Gradient Boosting, Light GBM, XGBoost는 모두 **부스팅**을 이용한 방법입니다.

그렇다면 배깅, 부스팅은 어떤 방법을 의미할까요?

----
## 배깅 (Bagging)

**배깅 (Bagging)**은 Bootstrap + Aggregating의 합성어로, "부트스트랩 샘플을 합친다!"라고 한 마디로 말씀드릴 수 있습니다. **부트스트랩 (Bootstrap)**이란 원래 통계학에서 표본 분포 (Sampling Distribution)을 구하기 위해 데이터를 여러 번 복원 추출 (resampling)하는 것을 의미합니다.

평균이 $\overline{X}$이고, 분산이 $\sigma^2/n$인 서로 독립인 $n$개의 데이터 $X_1,\ldots X_n$가 있을 때 이를 하나의 표본 (samples)이라 부릅니다. 그리고 보통 모집단 (population)의 모평균과 모분산을 추정하기 위해 표본 분포를 구합니다.
근데 표본 분포를 구하려면 또 $n$개의 표본들을 여러 개 구해야 하는데 사실상 쉽지가 않은 과정이죠.

이런 한계를 극복하기 위해 나온 과정이 부트스트랩입니다. 하나의 표본을 가지고 $B$ 번 복원 추출 (resampling) 한다면 이 **부트스트랩 샘플들의 분포가 표본 분포로 근사된다**는 것이 핵심입니다. 

자, 그럼 부트스트랩 샘플들을 어떻게 추출하는 지 그림을 통해 알아봅시다.
예를 들어 1,2,3,...,0까지의 값을 가진 $n=10$개인 하나의 표본이 있을 때 이 표본으로 $B=3$개의 부트스트랩 샘플을 추출하면 다음과 같습니다.

![](../../images/tree-bootstrap.png)

오른쪽의 세 개의 부트스트랩 샘플들은 왼쪽의 데이터 개수와 동일하지만, 개별 데이터가 중복되는 특징이 있습니다. 이렇게 
* 표본 크기와 똑같이 $B$번
* 복원 추출 (with replacement)
하는 것이 부트스트랩 샘플의 특징입니다.
만약 표본 평균을 구하는 것을 목표로 한다면 $B$ 번 추출한 부트스트랩 샘플들의 평균 $B$개들의 평균으로 근사할 수 있습니다. 

즉, 위의 예에서 3개의 부트스트랩 샘플에서 나온 평균 5, 4.9, 4.6의 합을 $B=3$으로 나누어주면 표본평균을 4.83으로 근사하게 됩니다. 



이러한 철학을 나무 모형에 적용한 게 배깅이고, 배깅을 이용한 대표적인 나무 알고리즘은 랜덤 포레스트입니다. 
랜덤 포레스트가 분류기를 만드는 과정은 다음과 같습니다. 

1. $N$개의 관측치와 $M$개의 피처를 가진 데이터가 있다 할 때 $B$개의 부트스트랩 샘플을 추출합니다. 각 샘플은 관측치 개수인 $N$개로 이루어져 있습니다.
2. 각 샘플에서 $M$개의 피처 중 **랜덤**하게 $d$개를 선택합니다. $d$는 보통 전채 특성 개수 $M$의 제곱근으로 주어집니다. 예를 들어 16개의 피처가 있다면 랜덤하게 4개의 피처를 선택합니다.
3. 이를 이용해 $B$개의 의사 결정 나무를 개별적으로 학습하고 최종적으로 모든 분류기가 보팅을 통해 예측 결정을 합니다.




Suppose there are N observations and M features. A sample from observation is selected randomly with replacement(Bootstrapping).
A subset of features are selected to create a model with sample of observations and subset of features.
Feature from the subset is selected which gives the best split on the training data.(Visit my blog on Decision Tree to know more of best split)
This is repeated to create many models and every model is trained in parallel
Prediction is given based on the aggregation of predictions from all the models.

