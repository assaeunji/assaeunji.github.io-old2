---
layout: post
title: "배깅 (Bagging)과 부스팅 (Boosting)"
date: 
categories: []
tag: []
comments: true
---



---
## Introduction

배깅과 부스팅을 알아보기 전에 **앙상블 학습**에 대한 개념을 알아야 합니다. 
**앙상블 학습** (Ensemble Learning)이란 여러 개의 분류기 (Classifier)를 생성하고 그 예측을 결합해 단일 분류기보다 정확한 최종적 예측을 도출해내는 기법을 의미합니다.

앙상블 학습의 유형은 크게 세 가지입니다 (그 외에도 스태킹을 포함한 다양한 앙상블 방법이 있다고 합니다).
* 보팅 (Voting)
* 배깅 (Bagging)
* 부스팅 (Boosting) 

나무 알고리즘 중 랜덤 포레스트 (Random Forest)는 **배깅**, 나머지 Gradient Boosting, Light GBM, XGBoost는 모두 **부스팅**을 이용한 방법입니다.

그렇다면 배깅, 부스팅은 어떤 방법을 의미할까요?

----
## 배깅 (Bagging)

**배깅 (Bagging)**은 Bootstrap + Aggregating의 합성어로, "부트스트랩 샘플을 합친다!"라고 한 마디로 말씀드릴 수 있습니다. **부트스트랩 (Bootstrap)**이란 원래 통계학에서 표본 분포 (Sampling Distribution)을 구하기 위해 데이터를 여러 번 복원 추출 (resampling)하는 것을 의미합니다.

평균이 $\overline{X}$이고, 분산이 $\sigma^2/n$인 서로 독립인 $n$개의 데이터 $X_1,\ldots X_n$가 있을 때 이를 하나의 표본 (samples)이라 부릅니다. 그리고 보통 모집단 (population)의 모평균과 모분산을 추정하기 위해 표본 분포를 구합니다.
근데 표본 분포를 구하려면 또 $n$개의 표본들을 여러 개 구해야 하는데 사실상 쉽지가 않은 과정이죠.

이런 한계를 극복하기 위해 나온 과정이 부트스트랩입니다. 하나의 표본을 가지고 $B$ 번 복원 추출 (resampling) 한다면 이 **부트스트랩 샘플들의 분포가 표본 분포로 근사된다**는 것이 핵심입니다. 

자, 그럼 부트스트랩 샘플들을 어떻게 추출하는 지 그림을 통해 알아봅시다.
예를 들어 1,2,3,...,0까지의 값을 가진 $n=10$개인 하나의 표본이 있을 때 이 표본으로 $B=3$개의 부트스트랩 샘플을 추출하면 다음과 같습니다.

![](../../images/tree-bootstrap.png)

오른쪽의 세 개의 부트스트랩 샘플들은 왼쪽의 데이터 개수와 동일하지만, 개별 데이터가 중복되는 특징이 있습니다. 이렇게 
* 표본 크기와 똑같이 $B$번
* 복원 추출 (with replacement)
하는 것이 부트스트랩 샘플의 특징입니다.
만약 표본 평균을 구하는 것을 목표로 한다면 $B$ 번 추출한 부트스트랩 샘플들의 평균 $B$개들의 평균으로 근사할 수 있습니다. 

즉, 위의 예에서 3개의 부트스트랩 샘플에서 나온 평균 5, 4.9, 4.6의 합을 $B=3$으로 나누어주면 표본평균을 4.83으로 근사하게 됩니다. 


이러한 철학을 나무 모형에 적용한 게 배깅이고, 배깅을 이용한 대표적인 나무 알고리즘은 랜덤 포레스트입니다. 
랜덤 포레스트가 분류기를 만드는 과정은 다음과 같습니다. 

1. $N$개의 관측치와 $M$개의 피처를 가진 데이터가 있다 할 때 $B$개의 부트스트랩 샘플을 추출합니다. 각 샘플은 중복 추출된 샘플입니다. 
    * 여기서 부트스트랩 샘플의 개수인 $B$는 `sklearn.ensemble.RandomForestClassifier`의 `n_estimators`로 조절하고 기본값은 100입니다.
    * 또한 각 부트스트랩 샘플의 크기는 `max_samples`로 조절하고, default는 데이터 개수인 $N$입니다.
2. 각 샘플에서 $M$개의 피처 중 **랜덤**하게 $d$개를 선택합니다. $d$는 보통 전채 피처 개수 $M$의 제곱근으로 주어집니다. 예를 들어 16개의 피처가 있다면 랜덤하게 4개의 피처를 선택합니다.
    * $d$는 `max_features`를 통해 조절합니다.
3. 이를 이용해 $B$개의 의사 결정 나무를 개별적으로 학습하고 최종적으로 모든 분류기가 보팅을 통해 예측 결정을 합니다.

랜덤포레스트가 부트스트랩 샘플로 학습한 나무들의 예측 결과를 합치는건 알겠는데... 왜 2.의 과정처럼 랜덤하게 일부의 피처를 선택할까요?
일부의 피처를 선택하지 않고 모든 피처를 사용한다면 **과적합** (overfitting) 문제가 있기 때문입니다. 

만약 피처가 16개인데 그 중 한 4개 정도만 유의미한 (dominating) 피처이고 나머지는 예측하는 데 큰 도움이 되지 않는 피처라 가정해봅시다.
16개의 피처를 모두 이용해서 부트스트랩 샘플 개수만큼 ($B$개) 의사 결정 나무를 학습하게 되면 결국 모든 학습 결과에 들어가는 분류 기준은 피처 4개만 관련이 있을 것이고 학습한 나무들은 다 비슷비슷하니까 correlation은 높아집니다. 이런 비슷한 나무들로 예측을 하게 되면 결정나무를 하나 학습하는 것보다 못한 예측을 할 수도 있겠죠.

따라서 과적합을 피하기 위해 


The idea of random forests is basically to build many decision trees (or other weak learners) that are decorrelated, so that their average is less prone to overfitting (reducing the variance). One way is subsampling of the training set. The reason why subsampling features can further decorrelate trees is, that if there are few dominating features, these features will be selected in many trees even for different subsamples, making the trees in the forest similar (correlated) again.

The lower the number of sampled features, the higher the decorrelation effect. On the other hand, the bias of a random forest is the same as the bias of any of the sampled trees (see for example Elements of Statistical Learning), but the randomization of random forests restrict the model, so that the bias is usually higher than a fully-grown (unpruned) tree. You are correct in that you can expect a higher bias if you sample fewer features. So, "feature bagging" really gives you a classical trade-off in bias and variance.