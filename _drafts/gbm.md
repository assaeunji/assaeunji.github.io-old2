---
layout: post
title: GBM (Gradeint Boosting Machines) 에 대한 자세한 설명 
date: 2020-08-30
categories: [ML]
tag: [tree algorithm,supervised-learning, boosting, gbm]
comments: true
photos:
    - "../../images/tree-titleimage.jpg"

---

* 이번 포스팅은 나무 모형 시리즈의 세 번째 글입니다. 이전 글은 [AdaBoost에 대한 자세한 설명](https://assaeunji.github.io/ml/2020-08-14-adaboost/)과 [배깅 (Bagging)과 부스팅 (Boosting)의 원리](https://assaeunji.github.io/ml/2020-08-06-tree/)에서 확인하실 수 있습니다.
* GBM은 LightGBM, CatBoost, XGBoost가 기반하고 있는 알고리즘이기 때문에 해당 원리를 아는 것이 중요합니다.


---
## Introduction



---
## Gradient Boosting Tree의 원리

Gradeint Boosting Tree (이하 GBM<sup>Gradient Boosting Model</sup>로 통칭하겠습니다)는 AdaBoost와 마찬가지로 Boosting 계열의 알고리즘이기 때문에 약한 학습기가 순차적


AdaBoost와 다른 점은 AdaBoost는 데이터와 약한 학습기의 가중치를 오차를 줄여나가는 방향으로 업데이트한다면 Gradient Boosting은 

---
## 



https://jinwonsohn.github.io/ml/nonparametric/2019/02/25/Boosting.html


https://3months.tistory.com/368

https://wikidocs.net/19037
